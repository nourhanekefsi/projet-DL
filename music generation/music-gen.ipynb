{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oX3_QDGXwF_N",
        "outputId": "8c37f7ee-d44c-4e9c-dbe5-b2592757d914"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['__MACOSX', 'deutschl']"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Path to the uploaded ZIP file\n",
        "zip_path = '/content/deutschl.zip'\n",
        "\n",
        "# Extract the contents\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content/dataset')\n",
        "\n",
        "# Verify contents\n",
        "os.listdir('/content/dataset')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "preprocessing"
      ],
      "metadata": {
        "id": "AuNURXqOw4sd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import music21 as m21\n",
        "import numpy as np\n",
        "import tensorflow.keras as keras\n",
        "\n",
        "KERN_DATASET_PATH = \"/content/dataset/deutschl\"  # Répertoire contenant les fichiers du dataset\n",
        "SAVE_DIR = \"/content/dataset/processed_dataset\"  # Répertoire où sauvegarder les fichiers encodés\n",
        "SINGLE_FILE_DATASET = \"/content/dataset/file_dataset\"  # Fichier unique pour toutes les chansons\n",
        "MAPPING_PATH = \"/content/dataset/mapping.json\"  # Fichier JSON pour les mappings\n",
        "SEQUENCE_LENGTH = 64  # Longueur des séquences pour l'entraînement\n",
        "\n",
        "# Durations acceptées (en quarts de temps)\n",
        "ACCEPTABLE_DURATIONS = [\n",
        "    0.25,  # 16ème de note\n",
        "    0.5,   # 8ème de note\n",
        "    0.75,\n",
        "    1.0,   # Noire\n",
        "    1.5,\n",
        "    2,     # Blanche\n",
        "    3,\n",
        "    4      # Ronde\n",
        "]\n",
        "\n",
        "def load_songs_in_kern(dataset_path):\n",
        "    \"\"\"Charge tous les morceaux au format Kern (.krn) à partir du dataset.\"\"\"\n",
        "    songs = []\n",
        "    for root, _, files in os.walk(dataset_path):\n",
        "        for file in files:\n",
        "            if file.endswith(\".krn\"):\n",
        "                file_path = os.path.join(root, file)\n",
        "                try:\n",
        "                    song = m21.converter.parse(file_path)\n",
        "                    songs.append(song)\n",
        "                except Exception as e:\n",
        "                    print(f\"Erreur lors du chargement de {file_path}: {e}\")\n",
        "    return songs\n",
        "\n",
        "\n",
        "def has_acceptable_durations(song, acceptable_durations):\n",
        "    \"\"\"Vérifie si toutes les durées dans la chanson sont acceptables.\"\"\"\n",
        "    for note in song.flat.notesAndRests:\n",
        "        if note.duration.quarterLength not in acceptable_durations:\n",
        "            return False\n",
        "    return True\n",
        "\n",
        "\n",
        "def transpose(song):\n",
        "    \"\"\"Transpose une chanson vers C majeur ou A mineur.\"\"\"\n",
        "    key = song.analyze(\"key\")\n",
        "    if key.mode == \"major\":\n",
        "        interval = m21.interval.Interval(key.tonic, m21.pitch.Pitch(\"C\"))\n",
        "    elif key.mode == \"minor\":\n",
        "        interval = m21.interval.Interval(key.tonic, m21.pitch.Pitch(\"A\"))\n",
        "    else:\n",
        "        return song  # Retourne la chanson sans transposition si la clé est indéterminée\n",
        "    return song.transpose(interval)\n",
        "\n",
        "\n",
        "def encode_song(song, time_step=0.25):\n",
        "    \"\"\"Encode une chanson en une série temporelle sous forme de liste.\"\"\"\n",
        "    encoded_song = []\n",
        "    for event in song.flat.notesAndRests:\n",
        "        if isinstance(event, m21.note.Note):\n",
        "            symbol = event.pitch.midi  # Note MIDI\n",
        "        elif isinstance(event, m21.note.Rest):\n",
        "            symbol = \"r\"  # Silence\n",
        "        else:\n",
        "            continue\n",
        "\n",
        "        steps = int(event.duration.quarterLength / time_step)\n",
        "        for step in range(steps):\n",
        "            if step == 0:\n",
        "                encoded_song.append(symbol)\n",
        "            else:\n",
        "                encoded_song.append(\"_\")\n",
        "    return \" \".join(map(str, encoded_song))\n",
        "\n",
        "\n",
        "def preprocess(dataset_path, save_dir):\n",
        "    \"\"\"Prétraite toutes les chansons dans le dataset.\"\"\"\n",
        "    if not os.path.exists(save_dir):\n",
        "        os.makedirs(save_dir)\n",
        "\n",
        "    songs = load_songs_in_kern(dataset_path)\n",
        "    print(f\"Nombre total de chansons chargées : {len(songs)}\")\n",
        "\n",
        "    for i, song in enumerate(songs):\n",
        "        if not has_acceptable_durations(song, ACCEPTABLE_DURATIONS):\n",
        "            continue\n",
        "\n",
        "        song = transpose(song)\n",
        "        encoded_song = encode_song(song)\n",
        "\n",
        "        # Sauvegarder la chanson prétraitée dans un fichier texte\n",
        "        file_name = f\"song_{i}.txt\"\n",
        "        save_path = os.path.join(save_dir, file_name)\n",
        "        with open(save_path, \"w\") as fp:\n",
        "            fp.write(encoded_song)\n",
        "\n",
        "        if i % 10 == 0:\n",
        "            print(f\"{i} chansons prétraitées sur {len(songs)}\")\n",
        "\n",
        "def load(file_path):\n",
        "    \"\"\"Charge une chanson encodée depuis un fichier.\"\"\"\n",
        "    with open(file_path, \"r\") as fp:\n",
        "        song = fp.read()\n",
        "    return song\n",
        "\n",
        "\n",
        "def create_single_file_dataset(dataset_path, file_dataset_path, sequence_length):\n",
        "    \"\"\"Crée un fichier unique contenant toutes les chansons encodées avec des délimiteurs.\"\"\"\n",
        "    new_song_delimiter = \"/ \" * sequence_length\n",
        "    songs = \"\"\n",
        "\n",
        "    for root, _, files in os.walk(dataset_path):\n",
        "        for file in files:\n",
        "            if file.endswith(\".txt\"):\n",
        "                file_path = os.path.join(root, file)\n",
        "                song = load(file_path)\n",
        "                songs += song + \" \" + new_song_delimiter\n",
        "\n",
        "    # Retirer le dernier espace vide\n",
        "    songs = songs.strip()\n",
        "\n",
        "    with open(file_dataset_path, \"w\") as fp:\n",
        "        fp.write(songs)\n",
        "    return songs\n",
        "\n",
        "def create_mapping(songs, mapping_path):\n",
        "\n",
        "    mappings = {}\n",
        "\n",
        "    # identify the vocabulary\n",
        "    songs = songs.split()\n",
        "    vocabulary = list(set(songs))\n",
        "\n",
        "    # create mappings\n",
        "    for i, symbol in enumerate(vocabulary):\n",
        "        mappings[symbol] = i\n",
        "\n",
        "    # save voabulary to a json file\n",
        "    with open(mapping_path, \"w\") as fp:\n",
        "        json.dump(mappings, fp, indent=4)\n",
        "\n",
        "\n",
        "def convert_songs_to_int(songs):\n",
        "    int_songs = []\n",
        "\n",
        "    # load mappings\n",
        "    with open(MAPPING_PATH, \"r\") as fp:\n",
        "        mappings = json.load(fp)\n",
        "\n",
        "    # transform songs string to list\n",
        "    songs = songs.split()\n",
        "\n",
        "    # map songs to int\n",
        "    for symbol in songs:\n",
        "        int_songs.append(mappings[symbol])\n",
        "\n",
        "    return int_songs\n",
        "\n",
        "\n",
        "def generate_training_sequences(sequence_length):\n",
        "\n",
        "    # load songs and map them to int\n",
        "    songs = load(SINGLE_FILE_DATASET)\n",
        "    int_songs = convert_songs_to_int(songs)\n",
        "\n",
        "    inputs = []\n",
        "    targets = []\n",
        "\n",
        "    # generate the training sequences\n",
        "    num_sequences = len(int_songs) - sequence_length\n",
        "    for i in range(num_sequences):\n",
        "        inputs.append(int_songs[i:i+sequence_length])\n",
        "        targets.append(int_songs[i+sequence_length])\n",
        "\n",
        "    # one-hot encode the sequences\n",
        "    vocabulary_size = len(set(int_songs))\n",
        "    # inputs size: (# of sequences, sequence length, vocabulary size)\n",
        "    inputs = keras.utils.to_categorical(inputs, num_classes=vocabulary_size)\n",
        "    targets = np.array(targets)\n",
        "\n",
        "    return inputs, targets\n",
        "\n",
        "def main():\n",
        "  preprocess(KERN_DATASET_PATH, SAVE_DIR)\n",
        "  print(\"Prétraitement terminé et dataset enregistré !\")\n",
        "  songs = create_single_file_dataset(SAVE_DIR, SINGLE_FILE_DATASET, SEQUENCE_LENGTH)\n",
        "  create_mapping(songs, MAPPING_PATH)\n",
        "  inputs, targets = generate_training_sequences(SEQUENCE_LENGTH)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ge_djOeEw575",
        "outputId": "cad2b1cb-3d46-42f1-ed72-38ac872cee28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "humdrum.spineParser: WARNING: Error in parsing event ('*MX') at position 8 for spine None: Incorrect meter: *MX found\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nombre total de chansons chargées : 628\n",
            "0 chansons prétraitées sur 628\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/music21/stream/base.py:3689: Music21DeprecationWarning: .flat is deprecated.  Call .flatten() instead\n",
            "  return self.iter().getElementsByClass(classFilterList)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10 chansons prétraitées sur 628\n",
            "20 chansons prétraitées sur 628\n",
            "30 chansons prétraitées sur 628\n",
            "40 chansons prétraitées sur 628\n",
            "50 chansons prétraitées sur 628\n",
            "60 chansons prétraitées sur 628\n",
            "70 chansons prétraitées sur 628\n",
            "80 chansons prétraitées sur 628\n",
            "90 chansons prétraitées sur 628\n",
            "100 chansons prétraitées sur 628\n",
            "110 chansons prétraitées sur 628\n",
            "120 chansons prétraitées sur 628\n",
            "130 chansons prétraitées sur 628\n",
            "140 chansons prétraitées sur 628\n",
            "150 chansons prétraitées sur 628\n",
            "160 chansons prétraitées sur 628\n",
            "170 chansons prétraitées sur 628\n",
            "180 chansons prétraitées sur 628\n",
            "190 chansons prétraitées sur 628\n",
            "200 chansons prétraitées sur 628\n",
            "210 chansons prétraitées sur 628\n",
            "230 chansons prétraitées sur 628\n",
            "240 chansons prétraitées sur 628\n",
            "250 chansons prétraitées sur 628\n",
            "260 chansons prétraitées sur 628\n",
            "270 chansons prétraitées sur 628\n",
            "280 chansons prétraitées sur 628\n",
            "290 chansons prétraitées sur 628\n",
            "300 chansons prétraitées sur 628\n",
            "310 chansons prétraitées sur 628\n",
            "320 chansons prétraitées sur 628\n",
            "330 chansons prétraitées sur 628\n",
            "340 chansons prétraitées sur 628\n",
            "350 chansons prétraitées sur 628\n",
            "360 chansons prétraitées sur 628\n",
            "370 chansons prétraitées sur 628\n",
            "380 chansons prétraitées sur 628\n",
            "390 chansons prétraitées sur 628\n",
            "400 chansons prétraitées sur 628\n",
            "410 chansons prétraitées sur 628\n",
            "420 chansons prétraitées sur 628\n",
            "430 chansons prétraitées sur 628\n",
            "440 chansons prétraitées sur 628\n",
            "450 chansons prétraitées sur 628\n",
            "460 chansons prétraitées sur 628\n",
            "470 chansons prétraitées sur 628\n",
            "480 chansons prétraitées sur 628\n",
            "490 chansons prétraitées sur 628\n",
            "500 chansons prétraitées sur 628\n",
            "510 chansons prétraitées sur 628\n",
            "520 chansons prétraitées sur 628\n",
            "530 chansons prétraitées sur 628\n",
            "540 chansons prétraitées sur 628\n",
            "560 chansons prétraitées sur 628\n",
            "570 chansons prétraitées sur 628\n",
            "580 chansons prétraitées sur 628\n",
            "590 chansons prétraitées sur 628\n",
            "600 chansons prétraitées sur 628\n",
            "610 chansons prétraitées sur 628\n",
            "620 chansons prétraitées sur 628\n",
            "Prétraitement terminé et dataset enregistré !\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "training"
      ],
      "metadata": {
        "id": "LyyRqQtuxxA_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import music21 as m21\n",
        "import numpy as np\n",
        "import tensorflow.keras as keras\n",
        "\n",
        "KERN_DATASET_PATH = \"/content/dataset/deutschl\"  # Dataset directory\n",
        "SAVE_DIR = \"/contentdataset/processed_dataset\"  # Directory to save encoded files\n",
        "SINGLE_FILE_DATASET = \"/content/dataset/file_dataset\"  # Single file containing all songs\n",
        "MAPPING_PATH = \"/content/dataset/mapping.json\"  # JSON file for mappings\n",
        "SEQUENCE_LENGTH = 64  # Sequence length for training\n",
        "\n",
        "OUTPUT_UNITS = 38  # Vocabulary size (ensure it matches your data)\n",
        "NUM_UNITS = [256]  # Number of units in LSTM layer\n",
        "LOSS = \"sparse_categorical_crossentropy\"  # Sparse crossentropy for integer targets\n",
        "LEARNING_RATE = 0.001\n",
        "EPOCHS = 50\n",
        "BATCH_SIZE = 64\n",
        "SAVE_MODEL_PATH = \"/content/model.h5\"\n",
        "\n",
        "def load(file_path):\n",
        "    \"\"\"Load an encoded song from a file.\"\"\"\n",
        "    with open(file_path, \"r\") as fp:\n",
        "        song = fp.read()\n",
        "    return song\n",
        "\n",
        "def convert_songs_to_int(songs):\n",
        "    \"\"\"Convert songs into integers using a mapping.\"\"\"\n",
        "    int_songs = []\n",
        "\n",
        "    # Load mappings\n",
        "    with open(MAPPING_PATH, \"r\") as fp:\n",
        "        mappings = json.load(fp)\n",
        "\n",
        "    # Transform song string to list\n",
        "    songs = songs.split()\n",
        "\n",
        "    # Map songs to integers\n",
        "    for symbol in songs:\n",
        "        int_songs.append(mappings[symbol])\n",
        "\n",
        "    return int_songs\n",
        "\n",
        "def generate_training_sequences(sequence_length):\n",
        "    \"\"\"Generate input and target sequences for training.\"\"\"\n",
        "    # Load songs and map them to integers\n",
        "    songs = load(SINGLE_FILE_DATASET)\n",
        "    int_songs = convert_songs_to_int(songs)\n",
        "\n",
        "    inputs = []\n",
        "    targets = []\n",
        "\n",
        "    # Generate the training sequences\n",
        "    num_sequences = len(int_songs) - sequence_length\n",
        "    for i in range(num_sequences):\n",
        "        inputs.append(int_songs[i:i + sequence_length])\n",
        "        targets.append(int_songs[i + sequence_length])\n",
        "\n",
        "    # One-hot encode the inputs\n",
        "    vocabulary_size = len(set(int_songs))\n",
        "    inputs = keras.utils.to_categorical(inputs, num_classes=vocabulary_size)  # Shape: (num_sequences, sequence_length, vocabulary_size)\n",
        "    targets = np.array(targets)  # Shape: (num_sequences,)\n",
        "\n",
        "    return inputs, targets, vocabulary_size\n",
        "\n",
        "def build_model(sequence_length, vocabulary_size, num_units, loss, learning_rate):\n",
        "\n",
        "    # Create the model architecture\n",
        "    input = keras.layers.Input(shape=(sequence_length, vocabulary_size))\n",
        "    x = keras.layers.LSTM(num_units[0], return_sequences=False)(input)\n",
        "    x = keras.layers.Dropout(0.2)(x)\n",
        "    output = keras.layers.Dense(vocabulary_size, activation=\"softmax\")(x)\n",
        "\n",
        "    model = keras.Model(input, output)\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(loss=loss,\n",
        "                  optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
        "                  metrics=[\"accuracy\"])\n",
        "\n",
        "    model.summary()\n",
        "\n",
        "    return model\n",
        "\n",
        "def train(sequence_length=SEQUENCE_LENGTH, num_units=NUM_UNITS, loss=LOSS, learning_rate=LEARNING_RATE):\n",
        "    \"\"\"Train and save the TensorFlow model.\"\"\"\n",
        "    # Generate the training sequences\n",
        "    inputs, targets, vocabulary_size = generate_training_sequences(sequence_length)\n",
        "\n",
        "    # Build the model\n",
        "    model = build_model(sequence_length, vocabulary_size, num_units, loss, learning_rate)\n",
        "\n",
        "    # Train the model\n",
        "    model.fit(inputs, targets, epochs=EPOCHS, batch_size=BATCH_SIZE)\n",
        "\n",
        "    # Save the model\n",
        "    model.save(SAVE_MODEL_PATH)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "LehFH5vpxyGx",
        "outputId": "e1774910-8838-478d-c4b9-bcb5578c933c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_2\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_2\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_2 (\u001b[38;5;33mInputLayer\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m42\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_2 (\u001b[38;5;33mLSTM\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 │         \u001b[38;5;34m306,176\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m42\u001b[0m)                  │          \u001b[38;5;34m10,794\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">42</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 │         <span style=\"color: #00af00; text-decoration-color: #00af00\">306,176</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">42</span>)                  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">10,794</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m316,970\u001b[0m (1.21 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">316,970</span> (1.21 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m316,970\u001b[0m (1.21 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">316,970</span> (1.21 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m2070/2070\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 7ms/step - accuracy: 0.7227 - loss: 1.0838\n",
            "Epoch 2/50\n",
            "\u001b[1m2070/2070\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.7695 - loss: 0.7144\n",
            "Epoch 3/50\n",
            "\u001b[1m2070/2070\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.7878 - loss: 0.6627\n",
            "Epoch 4/50\n",
            "\u001b[1m2070/2070\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 7ms/step - accuracy: 0.7959 - loss: 0.6367\n",
            "Epoch 5/50\n",
            "\u001b[1m2070/2070\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 7ms/step - accuracy: 0.7998 - loss: 0.6209\n",
            "Epoch 6/50\n",
            "\u001b[1m2070/2070\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.8062 - loss: 0.5985\n",
            "Epoch 7/50\n",
            "\u001b[1m2070/2070\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 8ms/step - accuracy: 0.8090 - loss: 0.5868\n",
            "Epoch 8/50\n",
            "\u001b[1m2070/2070\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 7ms/step - accuracy: 0.8150 - loss: 0.5689\n",
            "Epoch 9/50\n",
            "\u001b[1m2070/2070\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 7ms/step - accuracy: 0.8205 - loss: 0.5503\n",
            "Epoch 10/50\n",
            "\u001b[1m2070/2070\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 8ms/step - accuracy: 0.8231 - loss: 0.5341\n",
            "Epoch 11/50\n",
            "\u001b[1m2070/2070\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 7ms/step - accuracy: 0.8318 - loss: 0.5115\n",
            "Epoch 12/50\n",
            "\u001b[1m2070/2070\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.8353 - loss: 0.4959\n",
            "Epoch 13/50\n",
            "\u001b[1m2070/2070\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 8ms/step - accuracy: 0.8425 - loss: 0.4753\n",
            "Epoch 14/50\n",
            "\u001b[1m2070/2070\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.8491 - loss: 0.4565\n",
            "Epoch 15/50\n",
            "\u001b[1m2070/2070\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 7ms/step - accuracy: 0.8539 - loss: 0.4397\n",
            "Epoch 16/50\n",
            "\u001b[1m2070/2070\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 7ms/step - accuracy: 0.8617 - loss: 0.4190\n",
            "Epoch 17/50\n",
            "\u001b[1m2070/2070\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 7ms/step - accuracy: 0.8661 - loss: 0.4012\n",
            "Epoch 18/50\n",
            "\u001b[1m2070/2070\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 8ms/step - accuracy: 0.8695 - loss: 0.3954\n",
            "Epoch 19/50\n",
            "\u001b[1m2070/2070\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 8ms/step - accuracy: 0.8739 - loss: 0.3788\n",
            "Epoch 20/50\n",
            "\u001b[1m2070/2070\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.8827 - loss: 0.3517\n",
            "Epoch 21/50\n",
            "\u001b[1m2070/2070\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 8ms/step - accuracy: 0.8823 - loss: 0.3495\n",
            "Epoch 22/50\n",
            "\u001b[1m2070/2070\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 7ms/step - accuracy: 0.8905 - loss: 0.3282\n",
            "Epoch 23/50\n",
            "\u001b[1m2070/2070\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 7ms/step - accuracy: 0.8878 - loss: 0.3336\n",
            "Epoch 24/50\n",
            "\u001b[1m2070/2070\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 8ms/step - accuracy: 0.9010 - loss: 0.2985\n",
            "Epoch 25/50\n",
            "\u001b[1m2070/2070\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 7ms/step - accuracy: 0.9001 - loss: 0.2967\n",
            "Epoch 26/50\n",
            "\u001b[1m2070/2070\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 7ms/step - accuracy: 0.9005 - loss: 0.2978\n",
            "Epoch 27/50\n",
            "\u001b[1m2070/2070\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 8ms/step - accuracy: 0.9078 - loss: 0.2760\n",
            "Epoch 28/50\n",
            "\u001b[1m2070/2070\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.9071 - loss: 0.2753\n",
            "Epoch 29/50\n",
            "\u001b[1m2070/2070\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.9111 - loss: 0.2618\n",
            "Epoch 30/50\n",
            "\u001b[1m2070/2070\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.9131 - loss: 0.2546\n",
            "Epoch 31/50\n",
            "\u001b[1m2070/2070\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.9152 - loss: 0.2506\n",
            "Epoch 32/50\n",
            "\u001b[1m2070/2070\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 7ms/step - accuracy: 0.9142 - loss: 0.2549\n",
            "Epoch 33/50\n",
            "\u001b[1m2070/2070\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.9010 - loss: 0.2909\n",
            "Epoch 34/50\n",
            "\u001b[1m2070/2070\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 7ms/step - accuracy: 0.9196 - loss: 0.2396\n",
            "Epoch 35/50\n",
            "\u001b[1m2070/2070\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 7ms/step - accuracy: 0.9238 - loss: 0.2260\n",
            "Epoch 36/50\n",
            "\u001b[1m2070/2070\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.9190 - loss: 0.2399\n",
            "Epoch 37/50\n",
            "\u001b[1m2070/2070\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.9237 - loss: 0.2256\n",
            "Epoch 38/50\n",
            "\u001b[1m2070/2070\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 7ms/step - accuracy: 0.9292 - loss: 0.2106\n",
            "Epoch 39/50\n",
            "\u001b[1m2070/2070\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 7ms/step - accuracy: 0.9296 - loss: 0.2070\n",
            "Epoch 40/50\n",
            "\u001b[1m2070/2070\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.9289 - loss: 0.2110\n",
            "Epoch 41/50\n",
            "\u001b[1m2070/2070\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.9260 - loss: 0.2172\n",
            "Epoch 42/50\n",
            "\u001b[1m2070/2070\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 7ms/step - accuracy: 0.9332 - loss: 0.1983\n",
            "Epoch 43/50\n",
            "\u001b[1m2070/2070\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.9163 - loss: 0.2576\n",
            "Epoch 44/50\n",
            "\u001b[1m2070/2070\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 8ms/step - accuracy: 0.9338 - loss: 0.1954\n",
            "Epoch 45/50\n",
            "\u001b[1m2070/2070\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 7ms/step - accuracy: 0.9403 - loss: 0.1795\n",
            "Epoch 46/50\n",
            "\u001b[1m2070/2070\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.9361 - loss: 0.1867\n",
            "Epoch 47/50\n",
            "\u001b[1m2070/2070\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 7ms/step - accuracy: 0.9240 - loss: 0.2227\n",
            "Epoch 48/50\n",
            "\u001b[1m2070/2070\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 7ms/step - accuracy: 0.9392 - loss: 0.1790\n",
            "Epoch 49/50\n",
            "\u001b[1m2070/2070\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 8ms/step - accuracy: 0.9377 - loss: 0.1844\n",
            "Epoch 50/50\n",
            "\u001b[1m2070/2070\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 7ms/step - accuracy: 0.9372 - loss: 0.1825\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "melodie generation"
      ],
      "metadata": {
        "id": "zqQnUsHe4Z-i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import numpy as np\n",
        "import tensorflow.keras as keras\n",
        "import music21 as m21\n",
        "\n",
        "KERN_DATASET_PATH = \"/content/dataset/deutschl\"  # Dataset directory\n",
        "SAVE_DIR = \"/contentdataset/processed_dataset\"  # Directory to save encoded files\n",
        "SINGLE_FILE_DATASET = \"/content/dataset/file_dataset\"  # Single file containing all songs\n",
        "MAPPING_PATH = \"/content/mapping.json\"  # JSON file for mappings\n",
        "SEQUENCE_LENGTH = 64  # Sequence length for training\n",
        "\n",
        "class MelodyGenerator:\n",
        "\n",
        "    def __init__(self, model_path=\"model.h5\"):\n",
        "\n",
        "        self.model_path = model_path\n",
        "        self.model = keras.models.load_model(model_path)\n",
        "\n",
        "        with open(MAPPING_PATH, \"r\") as fp:\n",
        "            self._mappings = json.load(fp)\n",
        "\n",
        "        self._start_symbols = [\"/\"] * SEQUENCE_LENGTH\n",
        "\n",
        "\n",
        "    def generate_melody(self, seed, num_steps, max_sequence_length, temperature):\n",
        "\n",
        "        # create seed with start symbols\n",
        "        seed = seed.split()\n",
        "        melody = seed\n",
        "        seed = self._start_symbols + seed\n",
        "\n",
        "        # map seed to int\n",
        "        seed = [self._mappings[symbol] for symbol in seed]\n",
        "\n",
        "        for _ in range(num_steps):\n",
        "\n",
        "            # limit the seed to max_sequence_length\n",
        "            seed = seed[-max_sequence_length:]\n",
        "\n",
        "            # one-hot encode the seed\n",
        "            onehot_seed = keras.utils.to_categorical(seed, num_classes=len(self._mappings))\n",
        "            # (1, max_sequence_length, num of symbols in the vocabulary)\n",
        "            onehot_seed = onehot_seed[np.newaxis, ...]\n",
        "\n",
        "            # make a prediction\n",
        "            probabilities = self.model.predict(onehot_seed)[0]\n",
        "            # [0.1, 0.2, 0.1, 0.6] -> 1\n",
        "            output_int = self._sample_with_temperature(probabilities, temperature)\n",
        "\n",
        "            # update seed\n",
        "            seed.append(output_int)\n",
        "\n",
        "            # map int to our encoding\n",
        "            output_symbol = [k for k, v in self._mappings.items() if v == output_int][0]\n",
        "\n",
        "            # check whether we're at the end of a melody\n",
        "            if output_symbol == \"/\":\n",
        "                break\n",
        "\n",
        "            # update melody\n",
        "            melody.append(output_symbol)\n",
        "\n",
        "        return melody\n",
        "\n",
        "\n",
        "    def _sample_with_temperature(self, probabilites, temperature):\n",
        "\n",
        "        predictions = np.log(probabilites) / temperature\n",
        "        probabilites = np.exp(predictions) / np.sum(np.exp(predictions))\n",
        "\n",
        "        choices = range(len(probabilites)) # [0, 1, 2, 3]\n",
        "        index = np.random.choice(choices, p=probabilites)\n",
        "\n",
        "        return index\n",
        "\n",
        "\n",
        "    def save_melody(self, melody, step_duration=0.25, format=\"midi\", file_name=\"mel.mid\"):\n",
        "\n",
        "        # create a music21 stream\n",
        "        stream = m21.stream.Stream()\n",
        "\n",
        "        start_symbol = None\n",
        "        step_counter = 1\n",
        "\n",
        "        # parse all the symbols in the melody and create note/rest objects\n",
        "        for i, symbol in enumerate(melody):\n",
        "\n",
        "            # handle case in which we have a note/rest\n",
        "            if symbol != \"_\" or i + 1 == len(melody):\n",
        "\n",
        "                # ensure we're dealing with note/rest beyond the first one\n",
        "                if start_symbol is not None:\n",
        "\n",
        "                    quarter_length_duration = step_duration * step_counter # 0.25 * 4 = 1\n",
        "\n",
        "                    # handle rest\n",
        "                    if start_symbol == \"r\":\n",
        "                        m21_event = m21.note.Rest(quarterLength=quarter_length_duration)\n",
        "\n",
        "                    # handle note\n",
        "                    else:\n",
        "                        m21_event = m21.note.Note(int(start_symbol), quarterLength=quarter_length_duration)\n",
        "\n",
        "                    stream.append(m21_event)\n",
        "\n",
        "                    # reset the step counter\n",
        "                    step_counter = 1\n",
        "\n",
        "                start_symbol = symbol\n",
        "\n",
        "            # handle case in which we have a prolongation sign \"_\"\n",
        "            else:\n",
        "                step_counter += 1\n",
        "\n",
        "        # write the m21 stream to a midi file\n",
        "        stream.write(format, file_name)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    mg = MelodyGenerator()\n",
        "    seed = \"67 _ 64 _ 67 _ _ 65 64 _ 64 _ 64 _ _\"\n",
        "    seed2 = \"67 _ _ _ _ _ 65 _ 64 _ 62 _ 60 _ _ _\"\n",
        "    seed3 = \"60 _ _ 60 _ _ 62 _ 64 _ _ _ 65 _ _ _ _ _ 65 _ _ _ _ _ 67 _ _ 65 _ _ 64 62 _ 64 _ 64 _ 65 _ 64 _\"\n",
        "    seed4 = \"60 _ 62 _ _ 64 _ 65 _ 65 _ _ 65 _ 65 _ _ 65 _ _ 64 _ 64 _ _ 64 _ _ 65 _ 67 _ _ 69 _ _ 69 _ _ 69 _ _\"\n",
        "    melody = mg.generate_melody(seed4, 500, SEQUENCE_LENGTH, 0.9)\n",
        "    print(melody)\n",
        "    mg.save_melody(melody)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "78XebEgu4bfZ",
        "outputId": "db180dcd-f4a5-4dd0-fd65-da4eb298b123"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "['60', '_', '62', '_', '_', '64', '_', '65', '_', '65', '_', '_', '65', '_', '65', '_', '_', '65', '_', '_', '64', '_', '64', '_', '_', '64', '_', '_', '65', '_', '67', '_', '_', '69', '_', '_', '69', '_', '_', '69', '_', '_', '67', '_', '_', '_', '67', '_', '64', '_', '_', '64', '_', '_', '65', '67', '_', '_', '69', '72', '62', '_', '62', '_', '62', '_', '67', '_', '_', '71', '72', '71', '72', '_', '69', '_', '69', '_', '67', '_', '_', '_', '65', '_', '64', '_', '_', '_', '65', '_', '69', '_', '67', '_', '67', '_', '64', '_', '_', '_', '64', '_', '_', '_', '62', '_', '_', '_', '65', '_', '_', '_', '64', '_', '_', '_', '_', '_']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "interface"
      ],
      "metadata": {
        "id": "z9QvoLIeiXX4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FQ7jwoyFizen",
        "outputId": "320a350e-ec7c-4104-9192-a2931882d692"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gradio\n",
            "  Downloading gradio-5.13.1-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting aiofiles<24.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.7.1)\n",
            "Collecting fastapi<1.0,>=0.115.2 (from gradio)\n",
            "  Downloading fastapi-0.115.7-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.5.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting gradio-client==1.6.0 (from gradio)\n",
            "  Downloading gradio_client-1.6.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.25.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.27.1)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.5)\n",
            "Collecting markupsafe~=2.0 (from gradio)\n",
            "  Downloading MarkupSafe-2.1.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.26.4)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.15)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.1.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.10.5)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting python-multipart>=0.0.18 (from gradio)\n",
            "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\n",
            "Collecting ruff>=0.2.2 (from gradio)\n",
            "  Downloading ruff-0.9.3-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Collecting safehttpx<0.2.0,>=0.1.6 (from gradio)\n",
            "  Downloading safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
            "  Downloading starlette-0.45.3-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting tomlkit<0.14.0,>=0.12.0 (from gradio)\n",
            "  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.15.1)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.12.2)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.6.0->gradio) (2024.10.0)\n",
            "Requirement already satisfied: websockets<15.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.6.0->gradio) (14.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2024.12.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.25.1->gradio) (3.17.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.25.1->gradio) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.25.1->gradio) (4.67.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0->gradio) (2.27.2)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.25.1->gradio) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.25.1->gradio) (2.3.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Downloading gradio-5.13.1-py3-none-any.whl (57.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 MB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.6.0-py3-none-any.whl (321 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m321.8/321.8 kB\u001b[0m \u001b[31m30.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading fastapi-0.115.7-py3-none-any.whl (94 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading MarkupSafe-2.1.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (28 kB)\n",
            "Downloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
            "Downloading ruff-0.9.3-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m120.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\n",
            "Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading starlette-0.45.3-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\n",
            "Downloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ffmpy-0.5.0-py3-none-any.whl (6.0 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Installing collected packages: pydub, uvicorn, tomlkit, semantic-version, ruff, python-multipart, markupsafe, ffmpy, aiofiles, starlette, safehttpx, gradio-client, fastapi, gradio\n",
            "  Attempting uninstall: markupsafe\n",
            "    Found existing installation: MarkupSafe 3.0.2\n",
            "    Uninstalling MarkupSafe-3.0.2:\n",
            "      Successfully uninstalled MarkupSafe-3.0.2\n",
            "Successfully installed aiofiles-23.2.1 fastapi-0.115.7 ffmpy-0.5.0 gradio-5.13.1 gradio-client-1.6.0 markupsafe-2.1.5 pydub-0.25.1 python-multipart-0.0.20 ruff-0.9.3 safehttpx-0.1.6 semantic-version-2.10.0 starlette-0.45.3 tomlkit-0.13.2 uvicorn-0.34.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import json\n",
        "import numpy as np\n",
        "import tensorflow.keras as keras\n",
        "import music21 as m21\n",
        "import os\n",
        "\n",
        "KERN_DATASET_PATH = \"dataset/deutschl\"\n",
        "SAVE_DIR = \"dataset/processed_dataset\"\n",
        "SINGLE_FILE_DATASET = \"dataset/file_dataset\"\n",
        "MAPPING_PATH = \"/content/mapping.json\"\n",
        "SEQUENCE_LENGTH = 64\n",
        "\n",
        "class MelodyGenerator:\n",
        "\n",
        "    def __init__(self, model_path=\"model.h5\"):\n",
        "        self.model_path = model_path\n",
        "        self.model = keras.models.load_model(model_path)\n",
        "        with open(MAPPING_PATH, \"r\") as fp:\n",
        "            self._mappings = json.load(fp)\n",
        "        self._start_symbols = [\"/\"] * SEQUENCE_LENGTH\n",
        "\n",
        "    def generate_melody(self, seed, num_steps, max_sequence_length, temperature):\n",
        "        seed = seed.split()\n",
        "        melody = seed\n",
        "        seed = self._start_symbols + seed\n",
        "        seed = [self._mappings[symbol] for symbol in seed]\n",
        "\n",
        "        for _ in range(num_steps):\n",
        "            seed = seed[-max_sequence_length:]\n",
        "            onehot_seed = keras.utils.to_categorical(seed, num_classes=len(self._mappings))\n",
        "            onehot_seed = onehot_seed[np.newaxis, ...]\n",
        "            probabilities = self.model.predict(onehot_seed)[0]\n",
        "            output_int = self._sample_with_temperature(probabilities, temperature)\n",
        "            seed.append(output_int)\n",
        "            output_symbol = [k for k, v in self._mappings.items() if v == output_int][0]\n",
        "            if output_symbol == \"/\":\n",
        "                break\n",
        "            melody.append(output_symbol)\n",
        "        return melody\n",
        "\n",
        "    def _sample_with_temperature(self, probabilites, temperature):\n",
        "        predictions = np.log(probabilites) / temperature\n",
        "        probabilites = np.exp(predictions) / np.sum(np.exp(predictions))\n",
        "        choices = range(len(probabilites))\n",
        "        index = np.random.choice(choices, p=probabilites)\n",
        "        return index\n",
        "\n",
        "    def save_melody(self, melody, step_duration=0.25, format=\"midi\", file_name=\"mel.mid\"):\n",
        "        stream = m21.stream.Stream()\n",
        "        start_symbol = None\n",
        "        step_counter = 1\n",
        "        for i, symbol in enumerate(melody):\n",
        "            if symbol != \"_\" or i + 1 == len(melody):\n",
        "                if start_symbol is not None:\n",
        "                    quarter_length_duration = step_duration * step_counter\n",
        "                    if start_symbol == \"r\":\n",
        "                        m21_event = m21.note.Rest(quarterLength=quarter_length_duration)\n",
        "                    else:\n",
        "                        m21_event = m21.note.Note(int(start_symbol), quarterLength=quarter_length_duration)\n",
        "                    stream.append(m21_event)\n",
        "                    step_counter = 1\n",
        "                start_symbol = symbol\n",
        "            else:\n",
        "                step_counter += 1\n",
        "        stream.write(format, file_name)\n",
        "\n",
        "def generate_interface(seed, num_steps, max_sequence_length, temperature):\n",
        "    mg = MelodyGenerator()\n",
        "    melody = mg.generate_melody(seed, int(num_steps), int(max_sequence_length), float(temperature))\n",
        "    file_name = \"generated_melody.mid\"\n",
        "    mg.save_melody(melody, file_name=file_name)\n",
        "    return file_name\n",
        "\n",
        "# Gradio Interface\n",
        "interface = gr.Interface(\n",
        "    fn=generate_interface,\n",
        "    inputs=[\n",
        "        gr.Textbox(label=\"Seed Melody\", placeholder=\"Enter a seed melody, e.g., '67 _ 64 _ 67'\"),\n",
        "        gr.Number(label=\"Number of Steps\", value=500, precision=0),\n",
        "        gr.Number(label=\"Max Sequence Length\", value=64, precision=0),\n",
        "        gr.Slider(label=\"Temperature\", minimum=0.1, maximum=2.0, value=0.3)\n",
        "    ],\n",
        "    outputs=gr.File(label=\"Generated Melody (MIDI File)\"),\n",
        "    title=\"Melody Generator\",\n",
        "    description=\"Generate melodies using an AI model. Provide a seed melody and adjust parameters as needed.\"\n",
        ")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  interface.launch(debug=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "7PGmQ8lHiVfJ",
        "outputId": "a5f01727-a709-49b4-a41d-869cd1b38804"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://9d6d33488a48432bbc.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://9d6d33488a48432bbc.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 400ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n"
          ]
        }
      ]
    }
  ]
}